{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A user guid of `keras-unet-collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.3.0; Keras 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow {}; Keras {}'.format(tf.__version__, keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: importing `models` from `keras_unet_collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet_collection import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: defining your hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several hyper-parameter options are shared by several UNet-like models:\n",
    "\n",
    "* `inpust_size`: a tuple or list that defines the shape of input tensors. `models.resunet_a_2d` support int only, others supports NoneType. \n",
    "\n",
    "* `filter_num`: a list that defines the number of convolutional filters per down- and up-sampling blocks.\n",
    "\n",
    "* `n_labels`: number of output targets, e.g., `n_labels=2` for binary classification.\n",
    "\n",
    "* `activation`: the activation function of hidden layers. Available choices are \"ReLU\", \"LeakyReLU\", \"PReLU\", \"ELU\", \"GELU\", \"Snake\".\n",
    "\n",
    "* `output_activation`: the activation function of the output layer. Recommended choices are \"Softmax\", None (linear), \"Snake\".\n",
    "\n",
    "* `batch_norm`: if specified as True, all convolutional layers will be configured as stacked \"Conv2D-BN-Activation\" blocks.\n",
    "\n",
    "* `stack_num_down`: number of stacked convolutional layers per downsampling level.\n",
    "\n",
    "* `stack_num_up`: number of stacked convolutional layers (after concatenation) per upsampling level. \n",
    "\n",
    "* `pool`: if specified as False, the downsampling (encoding) blocks will be configured with stridden convolutional layers (2-by-2 linear kernels with 2 strides and activation function). Otherwise, (pool=True) max-pooling is used. \n",
    "\n",
    "* `unpool`: if specified as False, the upsampling (decoding) blocks will be configured with transpose convolutional layers (2-by-2 transpose kernels with 2 strides and activation function). Otherwise (unpool=True), reflective padding is used. \n",
    "    \n",
    "* `name`: user-specified prefix of the configured layer and model. Use `keras.models.Model.summary` to identify the exact name of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configuring your model (examples are provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**: A binary classification U-net with:\n",
    "\n",
    "1. Four down- and upsampliung levels.\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. One convolutional layer (after concatenation) per upsamling level.\n",
    "\n",
    "2. Gaussian Error Linear Unit (GELU) activcation, batch normalization.\n",
    "\n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through reflective padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = models.unet_2d((None, None, 3), [64, 128, 256, 512, 1024], n_labels=2,\n",
    "                      stack_num_down=2, stack_num_up=1,\n",
    "                      activation='GELU', output_activation='Softmax', \n",
    "                      batch_norm=True, pool=True, unpool=True, name='unet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**: An attention-Unet for single target regression with:\n",
    "\n",
    "1. three down- and upsampling levels.\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. Two convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "2. ReLU activation, batch normalization.\n",
    "\n",
    "3. Additive attention, ReLU attention activation.\n",
    "        \n",
    "4. downsampling through stride convolutional layers.\n",
    "\n",
    "5. Upsampling through transpose convolutional layers.   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_unet = models.att_unet_2d((None, None, 3), [64, 128, 256, 512], n_labels=1,\n",
    "                              stack_num_down=2, stack_num_up=2,\n",
    "                              activation='ReLU', atten_activation='ReLU', attention='add', output_activation=None, \n",
    "                              batch_norm=True, pool=False, unpool=False, name='att-unet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**: a modified U-net++ for three-label classification with:\n",
    "\n",
    "1. three down- and upsampling levels.\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. Two convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "2. LeakyReLU activation, no batch normalization.\n",
    "        \n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through transpose convolutional layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnet = models.unet_plus_2d((None, None, 3), [64, 128, 256, 512], n_labels=3,\n",
    "                           stack_num_down=2, stack_num_up=2,\n",
    "                           activation='LeakyReLU', output_activation='Softmax', \n",
    "                           batch_norm=False, pool=True, unpool=False, name='xnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4**: a R2U-net for binary classification with:\n",
    "\n",
    "1. three down- and upsampling levels.\n",
    "\n",
    "2. Two recurrent convolutional layers with two iterations per down- and upsampling level.\n",
    "\n",
    "2. ReLU activation, no batch normalization.\n",
    "        \n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through reflective padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_unet = models.r2_unet_2d((None, None, 3), [64, 128, 256, 512], n_labels=2,\n",
    "                            stack_num_down=2, stack_num_up=1, recur_num=2,\n",
    "                            activation='ReLU', output_activation='Softmax', \n",
    "                            batch_norm=True, pool=True, unpool=True, name='r2-unet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5**: a ResUnet-a for 16-label classification with:\n",
    "\n",
    "1. input size of (128, 128, 3)\n",
    "\n",
    "1. six downsampling levels followed by an Atrous Spatial Pyramid Pooling (ASPP) layer with 256 filters.\n",
    "\n",
    "1. six upsampling levels followed by an ASPP layer with 128 filters.\n",
    "\n",
    "2. dilation rates of {1, 3, 15, 31} for shallow layers, {1,3,15} for intermediate layers, and {1,} for deep layers.\n",
    "\n",
    "3. ReLU activation, batch normalization.\n",
    "\n",
    "4. Upsampling through reflective padding.\n",
    "\n",
    "* (Downsampling is fixed to strided convolutional layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received dilation rates: [1, 3, 15, 31]\n",
      "Expanding dilation rates:\n",
      "\tdepth-0, dilation_rate = [1, 3, 15, 31]\n",
      "\tdepth-1, dilation_rate = [1, 3, 15, 31]\n",
      "\tdepth-2, dilation_rate = [1, 3, 15]\n",
      "\tdepth-3, dilation_rate = [1, 3, 15]\n",
      "\tdepth-4, dilation_rate = [1]\n",
      "\tdepth-5, dilation_rate = [1]\n"
     ]
    }
   ],
   "source": [
    "resunet_a = models.resunet_a_2d((128, 128, 3), [32, 64, 128, 256, 512, 1024], \n",
    "                                dilation_num=[1, 3, 15, 31], \n",
    "                                n_labels=16, aspp_num_down=256, aspp_num_up=128, \n",
    "                                activation='ReLU', output_activation='Softmax', \n",
    "                                batch_norm=True, unpool=True, name='resunet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, delation rates can be specified per down- and uplampling level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resunet_a = models.resunet_a_2d((128, 128, 3), [32, 64, 128, 256, 512, 1024], \n",
    "                                dilation_num=[[1, 3, 15, 31], [1, 3, 15, 31], [1, 3, 15], [1, 3, 15], [1,], [1,],],\n",
    "                                n_labels=16, aspp_num_down=256, aspp_num_up=128, \n",
    "                                activation='ReLU', output_activation='Softmax', \n",
    "                                batch_norm=True, unpool=True, name='resunet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
